\chapter{Introduction}
\label{cha:introduction}

\dictum[Rachel Carson, \textit{Silent Spring} (1962)]{%
  The balance of nature is not a status quo; it is fluid, ever shifting, in a constant state of adjustment. \\ Man, too, is part of this balance.}%
\vskip 1em



\citep{bunne2021learning}
\acrfull{OT}



Optimal transport theory~\citep{santambrogio2015optimal} plays now a prominent role in the machine learning toolbox and has become within a few years the go-to framework to analyze, model, and solve an ever-increasing variety of tasks involving probability measures. This is best exemplified by its increasing importance to fitting generative models, where the goal is to learn a map~\citep{arjovsky2017wasserstein, genevay2018learning, salimans2018improving}, or more generally a diffusion \citep{song2020score, de2021diffusion} to morph a simple measure (e.g., Gaussian) onto a data distribution of interest (e.g., images). This is also apparent in the many applications that use OT to align probability measures that have since arisen, e.g., to transfer label knowledge between datasets~\citep{flamary2016optimal, singh2020model}, to analyze sampling schemes~\citep{dalalyan2017theoretical}, or study population trajectories~\citep{schiebinger2019optimal}.


We will start the tutorial by emphasizing the centrality of the optimal \textit{assignment} problem in various ML problems, motivated both by its conceptual simplicity and practical ability to compare sets of the same size. 
We will work out the mathematical foundations of optimal transport as an extension from these simple principles, recall its mathematical history from \citeauthor{monge1781histoire} (\citeyear{monge1781histoire}) and \citeauthor{kantorovich1942transfer} (\citeyear{kantorovich1942transfer}) to modern Fields medal winners \citeauthor{villani2009optimal} (\citeyear{villani2009optimal}) and \citeauthor{figalli2010optimal} (\citeyear{figalli2010optimal}). 
We focus next on the numerical resolution of the \citeauthor{kantorovich1942transfer} problem, its statistical~\citep{rigollet2022sample, genevay2018sample} and computational challenges, to motivate by-now classic algorithms \citep{cuturi2013sinkhorn, chizat2018scaling}, and their large-scale extensions \citep{altschuler2019massively, scetbon2021low}. We will also mention quadratic (Gromov) OT~\citep{memoli2011gromov} and its efficient computation~\citep{solomon2016entropic,scetbon2022low}.


We expect most of the audience to be familiar with the role of OT losses in the literature, e.g., for structured prediction \citep{frogner2015learning,janati2020multi} or generative model fitting \citep{yang2018scalable, arjovsky2017wasserstein, salimans2018improving, genevay2018learning}. Yet, we will gradually move away from the usual emphasis on OT distances (e.g., Wasserstein) towards a focus on the Monge \textit{map}, which provides an actionable way to flow from one probability distribution onto another. This section will start with a complete proof of the celebrated \citeauthor{Brenier1987} theorem for general costs, which will require an introduction to the notion of $c$-concavity. Once this quintessential result of OT theory is established, we will particularize it to translation-invariant costs and bridge it to the flurry of neural approaches that have been proposed in the literature. 
This comprises approaches that are a direct consequence of the \citeauthor{Brenier1987} theorem, to model Monge maps as gradients of convex functions, parameterized through input convex neural networks (ICNN) \citep{amos2017input, huang2021convex, makkuva2020optimal, korotin2021neural, lubeck2022neural, bunne2022supervised}, via regularizers \citep{uscidda2023monge}, amortized optimization \citep{amos2022amortizing, amos2022meta}, or entropic maps \citep{pooladian2021entropic, pooladian2023minimax, divol2022optimal, cuturi2023monge}.


\citet*{benamou2000computational} showed how the dynamic point of view offers an alternate and intuitive interpretation of optimal transport with links to fluid dynamics that surprisingly leads to a convex optimization problem that can be parameterized through normalizing flows \citep{tong2020trajectorynet}.
We will further highlight connections of OT to PDEs such as Fokker-Planck-like equations through the \citeauthor*{jordan1998variational} scheme: In recent works \citep{bunne2022proximal, alvarez2021optimizing, mokrov2021large, benamou2016augmented} 
it has found application in inferring the evolution of populations over time, crucial in many scientific disciplines when for instance, observing a population of cells in biology.
Beyond PDEs, we will explore the relation between the optimal transport problem and the Schr\"odinger bridge problem from stochastic control. It represents a key connection that has recently fueled the development of \acrlongpl{DSB} \citep{de2021diffusion, chen2021stochastic, bunne2022recovering, liu2022deep}. Compared to classical diffusion-based generative models \citep{daniels2021score, song2020score}, these algorithms allow interpolation between complex distributions. Extended to the Riemannian geometry \citep{thornton2022riemannian, de2022riemannian}, it has found applications in molecular dynamics \citep{holdijk2022path} and cell differentiation processes \citep{tong2023conditional, bunne2022recovering}.